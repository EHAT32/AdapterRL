{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f8abfe27",
      "metadata": {
        "id": "f8abfe27"
      },
      "source": [
        "# Извлечение признаков из nanoGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ae4fd2d1",
      "metadata": {
        "id": "ae4fd2d1"
      },
      "outputs": [],
      "source": [
        "from backbones.nanoGPT import GPT, GPTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "141eebd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "141eebd6",
        "outputId": "b9fb1d12-3291-4ab4-9204-51fc6c3dd7df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "number of parameters: 123.65M\n"
          ]
        }
      ],
      "source": [
        "model = GPT.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "11c5d79d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11c5d79d",
        "outputId": "043b8166-bd53-4e92-d172-87a1a7f31487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1024\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "text = \"You are a minecraft agent, you will be given an image history and actions history along with the instruction what to do. Your goal is to predict an action\" * 32\n",
        "token_num = len(enc.encode(text))\n",
        "print(token_num)\n",
        "print(token_num <= 1024)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef9eb2ac",
      "metadata": {
        "id": "ef9eb2ac"
      },
      "source": [
        "Как можем видеть, в гпт влезет довольно большая инструкция"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e856f9cb",
      "metadata": {
        "id": "e856f9cb"
      },
      "source": [
        "Теперь имплементируем простой извлекатель признаков. Для этого скорректируем исходник nanoGPT, модифицируем метод forward:\n",
        "\n",
        "```python\n",
        "def forward(self, idx, targets=None, only_latents = False):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        \n",
        "        if only_latents:\n",
        "            #let's return as tuple to keep the signature of the return\n",
        "            return None, None, x\n",
        "        \n",
        "        ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d5e94190",
      "metadata": {
        "id": "d5e94190"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class NanoGPTFeatureExtractor(nn.Module):\n",
        "    def __init__(self, device = \"cuda\", padding_id = 0, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.model = GPT.from_pretrained(\"gpt2\").to(device)\n",
        "        self.device = device\n",
        "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "        self.padding_id = padding_id\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, texts : list[str], pooling : str = \"last\", add_eos = False, max_tokens = 1024):\n",
        "      \"\"\"extract features from the batch of texts\n",
        "         extract them according to pooling:\n",
        "         pooling modes:\n",
        "          None or 'none' : return all tokens as is\n",
        "          'mean' : average of all tokens\n",
        "          'last' : last token\"\"\"\n",
        "      ids = [self.tokenizer.encode(t) for t in texts]\n",
        "      if add_eos:\n",
        "        eos = self.tokenizer.eot_token\n",
        "        ids = [x + [eos] for x in ids]\n",
        "      ids = [x[-max_tokens:] for x in ids]\n",
        "      #add padding for same length in batch\n",
        "      lengths = torch.tensor([len(id) for id in ids], device=self.device, dtype=torch.long)\n",
        "      B = len(ids)\n",
        "      T = int(lengths.max().item())\n",
        "      padded_ids = torch.full((B, T), fill_value=self.padding_id, device=self.device, dtype=torch.long)\n",
        "      for i, id in enumerate(ids):\n",
        "        padded_ids[i, :len(id)] = torch.tensor(id, device=self.device, dtype=torch.long)\n",
        "      _, _, latents = self.model(padded_ids, only_latents=True)\n",
        "      return self._pooling(lengths, latents, pooling)\n",
        "\n",
        "    def _pooling(self, lengths, latents, pooling):\n",
        "      if pooling is None or pooling == 'none':\n",
        "        return latents, lengths #B, T (padded), D\n",
        "      B = latents.size(0)\n",
        "      if pooling == 'last':\n",
        "        return latents[torch.arange(B, device=latents.device), lengths-1], lengths #B, D\n",
        "      T = latents.size(1)\n",
        "      if pooling == 'mean':\n",
        "        indices = torch.arange(T, device=latents.device).unsqueeze(0) # 1, T (padded)\n",
        "        mask = indices < lengths.unsqueeze(1) #B, T(padded)\n",
        "        mask = mask.float().unsqueeze(-1)\n",
        "        sum = (latents * mask).sum(dim=1)\n",
        "        denom = mask.sum(dim=1).clamp(min=1.0)\n",
        "        return sum / denom, lengths #B, D\n",
        "      return ValueError(f\"pooling can be either none | last | mean but {pooling} was given\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "OinzoBV_Sagt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OinzoBV_Sagt",
        "outputId": "5e11b613-4ccd-4cb1-cfd9-0ee42c05b13a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 768])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "texts = [\"hello, world!\", \"i love minecraft so much\"]\n",
        "\n",
        "extractor = NanoGPTFeatureExtractor(device=\"cpu\")\n",
        "\n",
        "latents, lengths = extractor(texts, pooling = \"last\")\n",
        "latents.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65108e97",
      "metadata": {},
      "source": [
        "Теперь этим кодом можем извлекать признаки из текстовых инструкций"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
